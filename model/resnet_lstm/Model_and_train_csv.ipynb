{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWMZn0YHDO2b",
        "outputId": "5499d96f-3274-4ddd-f86d-3955ac238bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['fake', 'real']\n",
            "Number of train samples: 7634\n",
            "Number of validation samples: 1909\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "# Path to the dataset root\n",
        "dataset_path = '/Users/dr.ake/Documents/GitHub/Deepfake-AI-SUTD/processed_dataset_frame'\n",
        "\n",
        "# Define the image transformations\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((im_size, im_size)),  # Resize all images to a fixed size\n",
        "    transforms.ToTensor(),                  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean, std)         # Normalize the tensor images\n",
        "])\n",
        "\n",
        "# Loading the dataset using ImageFolder\n",
        "dataset = ImageFolder(dataset_path, transform=transforms)\n",
        "\n",
        "# Splitting the dataset into train and validation subsets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "# Creating data loaders for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Example: Checking class names and some dataset info\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(\"Number of train samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(valid_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UtOXSqyBDRnD"
      },
      "outputs": [],
      "source": [
        "# #Model with feature visualization\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
        "#         super(Model, self).__init__()\n",
        "\n",
        "#         model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n",
        "#         self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "#         self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
        "#         self.relu = nn.LeakyReLU()\n",
        "#         self.dp = nn.Dropout(0.4)\n",
        "#         self.linear1 = nn.Linear(2048,num_classes)\n",
        "#         self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "#     def forward(self, x):\n",
        "#         batch_size,seq_length, c, h, w = x.shape\n",
        "#         x = x.view(batch_size * seq_length, c, h, w)\n",
        "#         fmap = self.model(x)\n",
        "#         x = self.avgpool(fmap)\n",
        "#         x = x.view(batch_size,seq_length,2048)\n",
        "#         x_lstm,_ = self.lstm(x,None)\n",
        "#         return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        model = models.resnext50_32x4d(pretrained=True)\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x expected to be in shape [batch_size, channels, height, width]\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the features out\n",
        "        x_lstm, _ = self.lstm(x.unsqueeze(1))  # Add a sequence dimension\n",
        "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYNhn10tDV90",
        "outputId": "2d8ac918-ebc4-4e9e-f30a-7c322d6f30ce"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import numpy as np\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNeXt50_32X4D_Weights\n",
        "\n",
        "model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "torch.Size([4, 3, 112, 112]) torch.Size([4])\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "for data, target in train_loader:\n",
        "    print(data.shape, target.shape)\n",
        "    break\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
        "    model.to(device)  # Ensure model is on the correct device\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                feature_map, outputs = model(inputs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during model forward pass: {e}\")\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct_predictions / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss, valid_acc = validate_model(model, valid_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, '\n",
        "              f'Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_acc:.4f}')\n",
        "\n",
        "def validate_model(model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            try:\n",
        "                feature_map, outputs = model(inputs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during model forward pass in validation: {e}\")\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    total_loss = running_loss / len(valid_loader.dataset)\n",
        "    total_acc = correct_predictions / len(valid_loader.dataset)\n",
        "    return total_loss, total_acc\n",
        "\n",
        "# Ensure your device setting is correct\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Initialize your model, criterion, and optimizer as before\n",
        "model = Model(num_classes=len(dataset.classes))\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Call the training function\n",
        "num_epochs = 10\n",
        "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "name": "Model_and_train_csv.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
